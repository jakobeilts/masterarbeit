{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "fac82e48ca9c5eec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T11:35:53.478645Z",
     "start_time": "2025-07-18T11:35:53.130128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from helper.list_of_all_html import urls\n",
    "from helper.academicCloudEmbeddings import AcademicCloudEmbeddings\n",
    "import streamlit as st"
   ],
   "id": "39003558a9965d67",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Crawl",
   "id": "225f8a79b2688271"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T11:35:53.540200Z",
     "start_time": "2025-07-18T11:35:53.489087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ── 1. Helper utilities ────────────────────────────────────────────────────────\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import requests, time\n",
    "from langchain.schema import Document          # or `from langchain.docstore.document import Document`\n",
    "\n",
    "def extract_table_as_text(table_tag):\n",
    "    \"\"\"\n",
    "    • Erkennt Überschriften in erster Zeile oder erster Spalte – auch ohne <th>.\n",
    "    • Wählt die Achse mit den meisten Einträgen als 'echter' Header.\n",
    "    • Fallback: Wenn beide Achsen leer ⇒ rohe Matrixzeilen ausgeben.\n",
    "    \"\"\"\n",
    "    # ---------- 1) Tabelle in Python‑Grid überführen ---------------------------\n",
    "    grid = []\n",
    "    for tr in table_tag.find_all(\"tr\"):\n",
    "        cells = [c.get_text(strip=True) for c in tr.find_all([\"td\", \"th\"])]\n",
    "        if any(cells):\n",
    "            grid.append(cells)\n",
    "\n",
    "    if not grid:\n",
    "        return []                                    # leere Tabelle\n",
    "\n",
    "    max_cols = max(len(r) for r in grid)\n",
    "    for r in grid:\n",
    "        r.extend([\"\"] * (max_cols - len(r)))         # kürzere Zeilen auffüllen\n",
    "\n",
    "    # ---------- 2) Zeilen‑ vs. Spalten‑Header zählen ---------------------------\n",
    "    first_row = grid[0]\n",
    "    first_col = [r[0] for r in grid]\n",
    "\n",
    "    row_header_count = sum(bool(c.strip()) for c in first_row)\n",
    "    col_header_count = sum(bool(c.strip()) for c in first_col)\n",
    "\n",
    "    # ---------- 3) Keine Header ⇒ rohe Matrix (Variante 1) ---------------------\n",
    "    if row_header_count == 0 and col_header_count == 0:\n",
    "        return [\" | \".join(r) for r in grid]\n",
    "\n",
    "    # ---------- 4) Achse mit den meisten Einträgen wird 'echt' -----------------\n",
    "    real_axis = \"row\" if row_header_count >= col_header_count else \"col\"\n",
    "\n",
    "    rows_out = []\n",
    "\n",
    "    if real_axis == \"row\":\n",
    "        real_headers   = first_row\n",
    "        other_headers  = first_col[1:]\n",
    "        for j, real_h in enumerate(real_headers):\n",
    "            if not real_h.strip():\n",
    "                continue\n",
    "            for i, other_h in enumerate(other_headers, start=1):\n",
    "                if not other_h.strip():\n",
    "                    continue\n",
    "                value = grid[i][j] if j < len(grid[i]) else \"\"\n",
    "                rows_out.append(f\"{real_h}: {other_h} = {value}\")\n",
    "\n",
    "    else:  # real_axis == \"col\"\n",
    "        real_headers  = first_col\n",
    "        other_headers = first_row[1:]\n",
    "        for i, real_h in enumerate(real_headers):\n",
    "            if i == 0 or not real_h.strip():\n",
    "                continue\n",
    "            for j, other_h in enumerate(other_headers, start=1):\n",
    "                if not other_h.strip():\n",
    "                    continue\n",
    "                value = grid[i][j] if j < len(grid[i]) else \"\"\n",
    "                rows_out.append(f\"{real_h}: {other_h} = {value}\")\n",
    "\n",
    "    return rows_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def replace_all_links_with_text_and_url(soup: BeautifulSoup):\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        label = a.get_text(strip=True)\n",
    "        href  = a.get(\"href\")\n",
    "        if href:\n",
    "            a.replace_with(f\"{label} ({href})\")\n",
    "        else:\n",
    "            a.replace_with(label)\n",
    "\n",
    "def remove_tools_divs(soup):\n",
    "    \"\"\"\n",
    "    Entfernt alle <div class=\"tools\">‑Elemente (inklusive ihres Inhalts) aus dem\n",
    "    BeautifulSoup‑Baum.\n",
    "    \"\"\"\n",
    "    for div in soup.select(\"div.tools\"):\n",
    "        div.decompose()\n",
    "\n",
    "def remove_ignored_parts(soup):\n",
    "    \"\"\"\n",
    "    Löscht aus der BeautifulSoup‑Instanz:\n",
    "      • <div class=\"tools\">\n",
    "      • <div class=\"docInfo\">\n",
    "      • <div id=\"dokuwiki__sitetools\">\n",
    "      • <nav id=\"dokuwiki__aside\">\n",
    "      • <a  href=\"#dokuwiki__content\">\n",
    "    \"\"\"\n",
    "    # 1. komplette Container entfernen\n",
    "    for sel in [\n",
    "        \"div.tools\",\n",
    "        \"div.docInfo\",\n",
    "        \"div#dokuwiki__sitetools\",\n",
    "        \"nav#dokuwiki__aside\",\n",
    "    ]:\n",
    "        for tag in soup.select(sel):\n",
    "            tag.decompose()\n",
    "\n",
    "    # 2. einzelne Anker entfernen\n",
    "    for a in soup.select('a[href=\"#dokuwiki__content\"]'):\n",
    "        a.decompose()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_inline_tags(soup_or_tag):\n",
    "    \"\"\"Remove inline presentational tags but keep their text.\"\"\"\n",
    "    for inner_tag in soup_or_tag.find_all([\"strong\", \"em\", \"span\", \"b\", \"i\", \"u\"]):\n",
    "        inner_tag.replace_with(inner_tag.get_text(strip=True))\n"
   ],
   "id": "d739e98c6e9b00c6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T11:35:53.660122Z",
     "start_time": "2025-07-18T11:35:53.657600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ── 2. Text‑extraction pipeline ────────────────────────────────────────────────\n",
    "def extract_visible_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # 1 Drop non‑visible nodes early\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # 2 Convert every <table> → readable text block\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        table_lines = extract_table_as_text(table)\n",
    "        table.replace_with(NavigableString(\"\\n\".join(table_lines)))\n",
    "\n",
    "    # 3 Replace <a> with \"label (href)\"\n",
    "    replace_all_links_with_text_and_url(soup)\n",
    "\n",
    "    # 4 Strip presentation‑only inline tags\n",
    "    clean_inline_tags(soup)\n",
    "    # 5  Tools‑Container löschen\n",
    "    remove_tools_divs(soup)\n",
    "    remove_ignored_parts(soup)\n",
    "    # 6 Collapse to plain text\n",
    "    visible_lines = [\n",
    "        line.strip() for line in soup.get_text(\"\\n\").splitlines() if line.strip()\n",
    "    ]\n",
    "    return \"\\n\".join(visible_lines)\n"
   ],
   "id": "7172d0f78d2f0d71",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-18T11:35:53.716912Z",
     "start_time": "2025-07-18T11:35:53.714424Z"
    }
   },
   "source": [
    "# ── 3. Crawler using the new extractor ─────────────────────────────────────────\n",
    "def crawl_urls(urls, delay: float = 0.4) -> list[Document]:\n",
    "    docs: list[Document] = []\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            r = requests.get(\n",
    "                url,\n",
    "                headers={\"User-Agent\": \"Mozilla/5.0\"},\n",
    "                timeout=15,\n",
    "            )\n",
    "            r.raise_for_status()\n",
    "\n",
    "            text = extract_visible_text(r.text)\n",
    "            docs.append(Document(page_content=text, metadata={\"url\": url}))\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f\"[!!] {url}: {exc}\")\n",
    "\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return docs\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Optional: Write docs to txt",
   "id": "d193a87c1a57283b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T11:38:28.916387Z",
     "start_time": "2025-07-18T11:35:53.739543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def save_docs_to_txt(docs, filename=\"data/crawled_pages.txt\"):\n",
    "    \"\"\"\n",
    "    Write a list of Document objects to one TXT file.\n",
    "    Each document is separated by a divider line.\n",
    "    \"\"\"\n",
    "    path = Path(filename)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            url = doc.metadata.get(\"url\", \"unknown‑url\")\n",
    "            f.write(f\"=== Document {i} ===\\n\")\n",
    "            f.write(f\"URL: {url}\\n\\n\")\n",
    "            f.write(doc.page_content)\n",
    "            f.write(\"\\n\\n\" + \"-\" * 80 + \"\\n\\n\")\n",
    "    print(f\"Wrote {len(docs)} documents → {path.resolve()}\")\n",
    "\n",
    "# after crawl_urls(...)\n",
    "docs = crawl_urls(urls)\n",
    "save_docs_to_txt(docs)"
   ],
   "id": "1bba4c18f468744",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 257 documents → /Users/jakobeilts/Development/Masterarbeit/2_increment/data/crawled_pages.txt\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chunking",
   "id": "2cdf5bce2e184b78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T11:48:31.149197Z",
     "start_time": "2025-07-18T11:38:29.037645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Seiten holen → Documents\n",
    "docs = crawl_urls(urls, delay=0.4)\n",
    "\n",
    "# 2) splitten – jede URL bleibt als metadata erhalten\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# 3) Embeddings und FAISS\n",
    "embedder = AcademicCloudEmbeddings(\n",
    "    api_key=st.secrets[\"GWDG_API_KEY\"],\n",
    "    url=st.secrets[\"BASE_URL_EMBEDDINGS\"],\n",
    ")\n",
    "store = FAISS.from_documents(chunks, embedder)\n",
    "store.save_local(\"faiss_wiki_index\")"
   ],
   "id": "360a9734592dab14",
   "outputs": [],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
