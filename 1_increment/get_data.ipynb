{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "fac82e48ca9c5eec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T13:57:13.531424Z",
     "start_time": "2025-07-15T13:57:13.118003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from helper.list_of_all_html import urls\n",
    "from helper.academicCloudEmbeddings import AcademicCloudEmbeddings\n",
    "import streamlit as st"
   ],
   "id": "39003558a9965d67",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Crawl",
   "id": "225f8a79b2688271"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-15T13:57:16.644320Z",
     "start_time": "2025-07-15T13:57:16.634810Z"
    }
   },
   "source": [
    "def extract_visible_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "    return \"\\n\".join(line.strip()\n",
    "                     for line in soup.get_text(\"\\n\").splitlines()\n",
    "                     if line.strip())\n",
    "\n",
    "def crawl_urls(urls, delay=0.4) -> list[Document]:\n",
    "    docs: list[Document] = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            r = requests.get(url,\n",
    "                             headers={\"User-Agent\": \"Mozilla/5.0\"},\n",
    "                             timeout=15)\n",
    "            r.raise_for_status()\n",
    "            text = extract_visible_text(r.text)\n",
    "            docs.append(Document(page_content=text,\n",
    "                                 metadata={\"url\": url}))\n",
    "        except Exception as exc:\n",
    "            print(f\"[!!] {url}: {exc}\")\n",
    "        time.sleep(delay)\n",
    "    return docs"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chunking",
   "id": "2cdf5bce2e184b78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-15T14:07:06.565003Z",
     "start_time": "2025-07-15T13:57:34.262905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Seiten holen → Documents\n",
    "docs = crawl_urls(urls, delay=0.4)\n",
    "\n",
    "# 2) splitten – jede URL bleibt als metadata erhalten\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# 3) Embeddings und FAISS\n",
    "embedder = AcademicCloudEmbeddings(\n",
    "    api_key=st.secrets[\"GWDG_API_KEY\"],\n",
    "    url=st.secrets[\"BASE_URL_EMBEDDINGS\"],\n",
    ")\n",
    "store = FAISS.from_documents(chunks, embedder)\n",
    "store.save_local(\"faiss_wiki_index\")"
   ],
   "id": "360a9734592dab14",
   "outputs": [],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
